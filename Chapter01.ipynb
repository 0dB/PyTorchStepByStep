{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# If you're using Google Colab, please run this #\n",
    "# cell to avoid errors importing torchviz       #\n",
    "#################################################\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to render the plots in this chapter\n",
    "from plots.chapter1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 864x360 with 2 Axes>,\n",
       " array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f34d2712b90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f34d1cfc810>],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure1(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Model's Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7421577700550976\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Computing the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (yhat - y_train)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.044811379650508 -1.8337537171510832\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[0.80119529] [0.04511107]\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "print(b, w)\n",
    "\n",
    "# Step 4 - Updates parameters using gradients and the learning rate\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to Step 1 and run observe how your parameters b and w change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[1.02354094] [1.96896411]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    b_grad = -2 * error.mean()\n",
    "    w_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[ 1.4889, -0.4679,  1.1240,  0.5487],\n",
      "         [-1.3980,  0.6968, -1.5616, -1.2452],\n",
      "         [-1.8481,  0.4273, -0.9301,  0.1610]],\n",
      "\n",
      "        [[ 1.2305, -0.0304,  0.2405,  1.0198],\n",
      "         [ 1.4939,  0.6187, -0.4445, -0.0112],\n",
      "         [ 0.2412, -1.5510, -0.3937,  0.7627]]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(scalar.size(), scalar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# We get a tensor with a different shape but it still is the SAME tensor\n",
    "same_matrix = matrix.view(1, 6)\n",
    "# If we change one of its elements...\n",
    "same_matrix[0, 1] = 2.\n",
    "# It changes both variables: matrix and same_matrix\n",
    "print(matrix)\n",
    "print(same_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 3., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvgodoy/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# We can use \"new_tensor\" method to REALLY copy it into a new one\n",
    "different_matrix = matrix.new_tensor(matrix.view(1, 6))\n",
    "# Now, if we change one of its elements...\n",
    "different_matrix[0, 1] = 3.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "# But we get a \"warning\" from PyTorch telling us to use \"clone()\" instead!\n",
    "print(matrix)\n",
    "print(different_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 4., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Lets follow PyTorch's suggestion and use \"clone\" method\n",
    "another_matrix = matrix.view(1, 6).clone().detach()\n",
    "# Again, if we change one of its elements...\n",
    "another_matrix[0, 1] = 4.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "print(matrix)\n",
    "print(another_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype, x_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor = x_train_tensor.float()\n",
    "float_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "# Modifies the numpy array\n",
    "dummy_array[1] = 0\n",
    "# Tensor gets modified too...\n",
    "dummy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7713], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tensor = torch.as_tensor(x_train).to(device)\n",
    "gpu_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "n_cudas = torch.cuda.device_count()\n",
    "for i in range(n_cudas):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them \n",
    "# into PyTorch's Tensors and then we send them to the chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bbb26eb4f53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mback_to_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "back_to_numpy = x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_to_numpy = x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"b\" and \"w\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], device='cuda:0', grad_fn=<CopyBackwards>) tensor([0.1288], device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(b, w)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], device='cuda:0', requires_grad=True) tensor([0.1288], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "b.requires_grad_()\n",
    "w.requires_grad_()\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FINAL\n",
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train_tensor - yhat)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "# No more manual computation of gradients! \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(error.requires_grad, yhat.requires_grad, b.requires_grad, w.requires_grad)\n",
    "print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.3881], device='cuda:0') tensor([-1.9439], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(b.grad, w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the two cells above one more time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will be placed *after* Step 4 (updating the parameters)\n",
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = y_train_tensor - yhat\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    # No more manual computation of gradients! \n",
    "    # b_grad = -2 * error.mean()\n",
    "    # w_grad = -2 * (x_tensor * error).mean()   \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad\n",
    "    # w = w - lr * w.grad\n",
    "    # print(b)\n",
    "\n",
    "    # SECOND ATTEMPT - using in-place Python assigment\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # b -= lr * b.grad\n",
    "    # w -= lr * w.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we used in the THIRD ATTEMPT..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 139831653666384 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139831653666384</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139831653663824 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139831653663824</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139831653663824&#45;&gt;139831653666384 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139831653663824&#45;&gt;139831653666384</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 139831653666576 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139831653666576</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139831653666576&#45;&gt;139831653666384 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139831653666576&#45;&gt;139831653666384</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 139831653665744 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139831653665744</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139831653665744&#45;&gt;139831653666576 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139831653665744&#45;&gt;139831653666576</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2d180b8890>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = y_train_tensor - yhat\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# We can try plotting the graph for any python variable: yhat, error, loss...\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"100pt\" height=\"157pt\"\n",
       " viewBox=\"0.00 0.00 100.00 157.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 153)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-153 96,-153 96,4 -4,4\"/>\n",
       "<!-- 139831673418768 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139831673418768</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"92,-21 0,-21 0,0 92,0 92,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673420176 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139831673420176</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"91.5,-78 .5,-78 .5,-57 91.5,-57 91.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673420176&#45;&gt;139831673418768 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139831673420176&#45;&gt;139831673418768</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-56.7787C46,-49.6134 46,-39.9517 46,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-31.1732 46,-21.1732 42.5001,-31.1732 49.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 139831673421648 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139831673421648</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"73,-149 19,-149 19,-114 73,-114 73,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139831673421648&#45;&gt;139831673420176 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139831673421648&#45;&gt;139831673420176</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-113.6724C46,-105.8405 46,-96.5893 46,-88.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-88.2234 46,-78.2234 42.5001,-88.2235 49.5001,-88.2234\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2d1938f1d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b_nograd + w * x_train_tensor\n",
    "\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"284pt\" height=\"399pt\"\n",
       " viewBox=\"0.00 0.00 284.00 399.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 395)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-395 280,-395 280,4 -4,4\"/>\n",
       "<!-- 139831673421776 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139831673421776</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"215,-21 123,-21 123,0 215,0 215,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"169\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673419600 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139831673419600</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"180,-78 82,-78 82,-57 180,-57 180,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"131\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673419600&#45;&gt;139831673421776 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139831673419600&#45;&gt;139831673421776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M138.1475,-56.7787C143.2429,-49.1357 150.2317,-38.6524 156.2694,-29.596\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.2497,-31.4352 161.8845,-21.1732 153.4253,-27.5522 159.2497,-31.4352\"/>\n",
       "</g>\n",
       "<!-- 139831673355408 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139831673355408</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159.5,-135 66.5,-135 66.5,-114 159.5,-114 159.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673355408&#45;&gt;139831673419600 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139831673355408&#45;&gt;139831673419600</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M116.3857,-113.7787C118.6987,-106.4542 121.8354,-96.5211 124.61,-87.7352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.9557,-88.763 127.6295,-78.1732 121.2806,-86.655 127.9557,-88.763\"/>\n",
       "</g>\n",
       "<!-- 139831673354064 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139831673354064</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"158,-192 68,-192 68,-171 158,-171 158,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673354064&#45;&gt;139831673355408 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139831673354064&#45;&gt;139831673355408</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113,-170.7787C113,-163.6134 113,-153.9517 113,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.5001,-145.1732 113,-135.1732 109.5001,-145.1732 116.5001,-145.1732\"/>\n",
       "</g>\n",
       "<!-- 139831673352784 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139831673352784</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159,-249 67,-249 67,-228 159,-228 159,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673352784&#45;&gt;139831673354064 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139831673352784&#45;&gt;139831673354064</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113,-227.7787C113,-220.6134 113,-210.9517 113,-202.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.5001,-202.1732 113,-192.1732 109.5001,-202.1732 116.5001,-202.1732\"/>\n",
       "</g>\n",
       "<!-- 139831673356048 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139831673356048</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-320 0,-320 0,-285 54,-285 54,-320\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-292.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139831673356048&#45;&gt;139831673352784 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139831673356048&#45;&gt;139831673352784</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.9558,-284.6724C63.3538,-275.446 78.3987,-264.2498 90.5683,-255.1934\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"92.9097,-257.8138 98.8425,-249.0358 88.7306,-252.1981 92.9097,-257.8138\"/>\n",
       "</g>\n",
       "<!-- 139831673352976 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139831673352976</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-313 72.5,-313 72.5,-292 163.5,-292 163.5,-313\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-299.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673352976&#45;&gt;139831673352784 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139831673352976&#45;&gt;139831673352784</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M117.1744,-291.9317C116.4837,-283.0913 115.4775,-270.2122 114.6261,-259.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.1119,-258.9949 113.8436,-249.2979 111.1332,-259.5402 118.1119,-258.9949\"/>\n",
       "</g>\n",
       "<!-- 139831673420048 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139831673420048</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-391 145,-391 145,-356 199,-356 199,-391\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-363.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139831673420048&#45;&gt;139831673352976 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139831673420048&#45;&gt;139831673352976</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M158.6517,-355.9494C150.6484,-345.4266 140.4734,-332.0484 132.3053,-321.3089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.8474,-318.8695 126.0078,-313.0288 129.2757,-323.1071 134.8474,-318.8695\"/>\n",
       "</g>\n",
       "<!-- 139831673353296 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139831673353296</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272.5,-313 181.5,-313 181.5,-292 272.5,-292 272.5,-313\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-299.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673420048&#45;&gt;139831673353296 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139831673420048&#45;&gt;139831673353296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M185.5955,-355.9494C193.8285,-345.3214 204.3179,-331.7806 212.6787,-320.9875\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.4868,-323.0778 218.8439,-313.0288 209.9529,-318.7909 215.4868,-323.0778\"/>\n",
       "</g>\n",
       "<!-- 139831673420240 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139831673420240</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"276,-135 178,-135 178,-114 276,-114 276,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673420240&#45;&gt;139831673421776 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139831673420240&#45;&gt;139831673421776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M221.6475,-113.9795C211.9855,-94.9888 191.4875,-54.6995 179.1119,-30.3751\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"182.1629,-28.6533 174.5088,-21.3276 175.9239,-31.8275 182.1629,-28.6533\"/>\n",
       "</g>\n",
       "<!-- 139831673353360 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139831673353360</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-249 182,-249 182,-228 272,-228 272,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 139831673353360&#45;&gt;139831673420240 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139831673353360&#45;&gt;139831673420240</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227,-227.9795C227,-209.242 227,-169.7701 227,-145.3565\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.5001,-145.3276 227,-135.3276 223.5001,-145.3277 230.5001,-145.3276\"/>\n",
       "</g>\n",
       "<!-- 139831673353296&#45;&gt;139831673353360 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139831673353296&#45;&gt;139831673353360</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227,-291.9317C227,-283.0913 227,-270.2122 227,-259.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.5001,-259.2979 227,-249.2979 223.5001,-259.2979 230.5001,-259.2979\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2d1937f510>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b + w * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# this makes no sense!!\n",
    "if loss > 0:\n",
    "    yhat2 = w * x_train_tensor\n",
    "    error2 = y_train_tensor - yhat2\n",
    "    \n",
    "# neither does this :-)\n",
    "loss += error2.mean()\n",
    "\n",
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step / zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = y_train_tensor - yhat\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1700)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a random example to illustrate the loss function\n",
    "predictions = torch.tensor([0.5, 1.0])\n",
    "labels = torch.tensor([2.0, 1.3])\n",
    "loss_fn(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # No more manual loss!\n",
    "    # error = y_train_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-8664a4f00328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You will NOT get an error here if you do not have a GPU!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# You will NOT get an error here if you do not have a GPU!\n",
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.00804466, dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008044655434787273 0.008044655434787273\n"
     ]
    }
   ],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True), Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "dummy = ManualLinearRegression()\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [139831653594928, 139831656256048]}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model and sends it to the device\n",
    "dummy = ManualLinearRegression().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0235], device='cuda:0')), ('w', tensor([1.9690], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # What is this?!?\n",
    "\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Never forget to include model.train() in your training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1, out_features=1, bias=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1)\n",
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[-0.2191]])), ('bias', tensor([0.2018]))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0.8300], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = MyLinearRegression().to(device)\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('linear.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
       "                      [ 0.5304, -0.1265,  0.1165],\n",
       "                      [-0.2811,  0.3391,  0.5090],\n",
       "                      [-0.4236,  0.5018,  0.1081],\n",
       "                      [ 0.4266,  0.0782,  0.2784]], device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')),\n",
       "             ('1.bias', tensor([-0.2062], device='cuda:0'))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer1): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (layer2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential()\n",
    "model.add_module('layer1', nn.Linear(3, 5))\n",
    "model.add_module('layer2', nn.Linear(5, 1))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "# If you're running this in Google Colab, it needs to create the folders\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    folders = ['data_preparation', 'model_configuration', 'model_training']\n",
    "\n",
    "    for folder in folders:\n",
    "        try:\n",
    "            os.mkdir(folder)\n",
    "        except OSError as e:\n",
    "            e.errno\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "                \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_preparation/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparation/v0.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparation/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_configuration/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v0.py\n",
    "\n",
    "# This is redundant now, but it won't be when we introduce Datasets...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v0.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]], device='cuda:0')), ('0.bias', tensor([1.0235], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
