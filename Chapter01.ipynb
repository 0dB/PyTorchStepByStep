{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# If you're using Google Colab, please run this #\n",
    "# cell to avoid errors importing torchviz       #\n",
    "#################################################\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Generated Data - Validation')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzEAAAFuCAYAAACx0ZaLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzde3hU1b3/8U8uhAQIBDAkAUkIiAgoVrAoICKXgliEiEYUrNrWWAU8PZ6qiCcepaYK4rWiiFCttOAFQQrKpaLIHTSgokAhbeJwMYGQcAkhISHJ74/8Zshk9iSZZG575v16Hp7HrNmz99o7l+V31vp+V8jJkyerBAAAAAAmEerrDgAAAACAKwhiAAAAAJgKQQwAAAAAUyGIAQAAAGAqBDEAAAAATIUgBgAAAICpEMQAbrRp0ybFxMTowQcf9HVX0ARdu3bVgAEDfN0NACbDGGAevXr10lVXXWXXtnDhQsXExOiDDz5o8Hnuv/9+xcTEaNu2be7uop0bb7xR7du39+g1zIYgJgDk5OQoPT1dQ4YMUZcuXXTRRRcpKSlJQ4cO1fTp07Vr1y5fd9GnYmJidMUVV/i6G4YsFotiYmLs/sXFxemSSy7RsGHD9PDDD+vLL79UVZV7tnNatGiRYmJi9Nxzz7nlfO50xRVXODyLuv754z0AvsAYUDfGgAv8eQx45plnFBMToyeeeKLeYzMyMhQTE6P09HQv9Mw7brzxRsXExOjIkSO+7opphPu6A2ial156SX/6059UUVGhPn36aPz48Wrbtq2Kioq0d+9evf3225o7d65mzJih3//+977uLpxo3bq17ZO7iooKnTx5Uvv27dPf/vY3vfPOO7rmmms0b948denSxbcd9aAHH3xQp06dsmv79NNP9cMPP+imm25y+J+Q6667zmN9WbdunUJD+YwH/o8xIDAwBkh33323Xn75Zb3//vt66qmn1Lx5c8PjKioqtHjxYknSvffe69Y+jBs3Ttdee63i4+Pdel53mD9/vkpLS33dDb9CEGNiL7/8sv74xz/q4osv1oIFC3Tttdc6HFNYWKg333xTRUVFPughGqpNmzaaPn26Q3teXp4eeeQRffLJJxo7dqzWr18fsNPJkydPdmg7ePCgfvjhB/3yl7/UpEmTvNaXrl27eu1aQGMxBgQOxgDZZg8///xzffLJJ7r11lsNj1u7dq1++uknXXfddbrkkkvc2oc2bdqoTZs2bj2nu3Tu3NnXXfA7fNRoUhaLRc8++6wiIiK0ZMkSw8FLktq1a6cnnnhCjz/+uMNrlZWVWrhwoUaNGqXExETFxcVpwIABeumll1RWVuZwvHVK/uzZs3ryySd1+eWXq0OHDrrqqqv08ssvO53u/vbbb/Wb3/xGl112mWJjY9WjRw/df//9ys7Odjj2wQcfVExMjDZt2qRFixZpyJAh6tixo+1T97KyMr311lu67bbbbNdPSkrS2LFjtXbtWrtzWdcmS9KhQ4fsputrr1fOzs7WQw89ZDtnt27dNGnSJH377beG93Ts2DFNnTpV3bt3V3x8vK677jotWrTI8NimiI+P17vvvqtBgwbp4MGDeumll+xe//e//62nn35aN9xwg7p166YOHTro8ssv13/913/p0KFDdsc++OCDmjJliiRp1qxZds9j06ZNkqRTp07p1Vdf1ZgxY9SzZ0/FxsaqW7duuuOOO7Rjxw6335873H333YqJidE333yjd955R9ddd53i4+N14403SpJKSkr0xhtv6JZbblHv3r3VoUMHJScna/z48Vq/fr3hOY1yYt566y3FxMTotdde086dOzV+/HglJiaqU6dOuvnmm4N+yQ68izGAMUAKvDHAOrPy7rvvOj3G+tqvf/1rW9u5c+c0b9483XrrrbbvYZcuXZSSkqLPPvuswdevKyfm888/16hRo9SxY0d16dJFkyZNUlZWltNzrVixQvfdd5/69u2rjh07qlOnThoyZIjefPNNVVZW2o47f/68YmJitH37dklS7969bd+Xmjk7znJiKisr9fbbb2vYsGG6+OKL1bFjRw0ePFhz5sxReXm5w/G9evVS+/btVV5ertmzZ+uqq66y/dw89dRThr/7/oqZGJNatGiRysvLlZqaqp49e9Z7fHi4/bf6/Pnzuuuuu7RmzRpdcskluvXWW9W8eXNt2bJFf/zjH7VhwwYtXbrU8H3jx49XXl6eRowYofDwcH366aeaMWOGSkpKHNayfvjhh5o8ebIiIiI0evRoderUSdnZ2Vq6dKnWrFmjTz75RH369HHo72uvvaaNGzdq9OjRuuGGG3Tu3DlJ0okTJ/T444/rmmuu0dChQ3XRRRcpLy9Pq1at0oQJE/TKK6/Y/ggmJiZq2rRpmjVrlt1UvSS7pUkbNmzQpEmTVFpaqlGjRqlbt27Kzc3VypUrtW7dOi1evFjDhw+3HV9YWKiRI0fqxx9/1DXXXKOBAwfaPi0bMmRIvd8LV4WFhenRRx/Vli1btGTJEv3pT3+yvbZy5Uq9/fbbGjx4sPr376+IiAjbEoTVq1fryy+/VKdOnSRJv/zlL3Xq1CmtWrVKgwYNsluOlZiYKEk6cOCAMjIyNHDgQI0aNUoxMTE6dOiQVq1apc8++0zvvfeeRo4c6fZ7dIfnnntOmzdv1ujRo+2+X7m5uXryySd1zTXXaPjw4Wrfvr2OHDmiNWvWaPz48Zo3b55uv/32Bl9n27ZtysjI0ODBg3X33Xfrxx9/1Keffqqbb75ZW7ZsCejlHvAfjAGMAVLgjQGjR49WfHy8Nm3apOzsbIdZ8SNHjmjdunVq3769br75Zlv78ePHNX36dLufi9zcXK1atUqpqan685//rLvvvrvR/Vq2bJnuu+8+RURE6JZbblF8fLy2b9+uX/ziF05//6xL4q6++mp17NhRp06d0pdffqnHH39c33zzjebNmydJCg0N1bRp07Ro0SIdPnxYkydPVnR0tCSpbdu29fbtvvvu07Jly9SpUydNnDhR4eHhWr16tdLT0/XFF1/oww8/dPg9lqTf/OY3yszM1PDhw9WyZUv985//1KuvvqqCggLNmTOn0c/Km0JOnjzpnmwxeNXYsWO1cePGRv9izp49W3/605+UlpammTNnKiwsTFJ1RP/www/r3Xff1cyZM/XAAw/Y3mP9RGvUqFF69913FRkZKUnKz89Xv379JEn/+c9/1KxZM0nVn2wNGDBACQkJWrVqlTp27Gg716ZNm5SSkqLevXtr48aNtvYHH3xQ7733nlq0aKE1a9Y4DG7nzp3T8ePHbX+UrU6ePKlRo0bp6NGj2rdvn6Kiouz63blzZ33//fcOz+HUqVO66qqrVFVVpdWrV+uyyy6zvbZ//34NHz5crVq10nfffWdbn/v73/9e7777rtLS0jR79mzb8d99951GjBih8vJy3XnnnZo7d2693weLxaIrr7zSaf9q3nenTp10/vx5fffdd0pKSpIk/fTTT2rfvr3D2uHPPvtMEyZM0D333KOXX37Z1r5o0SJNmTJF06ZNM1y6cOrUKZ0/f97h056DBw9qxIgRatOmjb7++ut678tdrD8Pr7/+utPlZHfffbdWrFih1q1b67PPPlOPHj3sXj979qxOnz7tsMb5+PHjGjFihEpKSrRnzx67P/Jdu3ZVXFycXbWZt956S4899pgk6W9/+5vdAPraa6/pySef1EMPPaRnnnmmyfcN1IcxgDFACswxICMjQy+88IL++7//W08//bTdazNnztTMmTMd/taWlpaqsLDQ7mdMqg56R44cqYKCAu3bt8/uOfXq1UvNmzfXN998Y2tbuHCh/uu//kvz5s3ThAkTJEmnT5/WFVdcoeLiYn322Wd2syPTp0+3fZ9Xr15tN4Ofk5Oj5ORku/5UVFTo/vvv19KlS7V+/XqHmZbt27drz549Dj/f1te//vprFRQU2Nref/99PfDAA+rTp48+/fRTW/Bz7tw5jR8/Xlu2bNGf/vQn2wyc9b5/+uknXX311froo49sv9dnzpzRoEGDdPjwYf3rX/9SbGysQx/8DcvJTOro0aOS5PALK1VPmz/33HN2/1577TXb65WVlXrzzTcVGxur5557zjZ4SdWfCPzxj39USEiI0xKDs2bNsg1ekhQbG6tf/vKXOn36tN3U6l/+8hedO3dOzz77rEM/Bw8erNGjR2v37t3at2+fwzXuvvtuw0/nmjdvbvjLHRMTo7vuuksnT550aVnP+++/r8LCQk2bNs1u8JKkHj166O6771ZeXp6+/PJLSVJ5ebmWLFmili1b6n//93/tjr/yyitd+kTfFc2bN7d9InP8+HFbe8eOHQ2TH3/xi1/osssu0xdffOHSddq0aWM4XZ2YmKhx48YpKyvLYYmCv7j//vsdAhhJatGihWGS5kUXXaQ77rhDR48e1Q8//NDg6/ziF7+wC2CkC0sgWFIGb2EMsMcYYM/MY8Ddd9+t0NBQLV682G45VGVlpf7+979Lku655x6790RGRhr+LrRt21aTJk1SYWGh06WB9fnkk0906tQp3XbbbQ4lmR9//HFb4FBb7QBGqp5Vs84Iuvq9MWJ9Hk8//bRdP5o3b65nn31WUnVgZmTGjBm2AEaSWrVqpdTUVFVUVOi7775rct+8geVkJmVdexwSEuLw2uHDhzVr1iy7tg4dOuihhx6SVL2GtqCgQMnJyXafItUUFRVluNazTZs2hstlrIPKyZMnbW3W9bNbt241/IXIz8+XVD19XXs69uqrrzbslyTt27dPf/7zn7V161bl5eXZlhlY5ebmOn1vbdY+7tmzx7Dk5L///W9bH0eNGqUDBw7o7Nmz6t+/v90vv9WgQYM8si66pprf86qqKn344YdavHixfvjhB508eVIVFRW21yMiIlw+//bt2/Xmm2/q66+/Vn5+vsP62Nzc3HoTDDdt2qTNmzfbtSUmJno0Od/6SbCR7777Tq+//rq2bdumY8eOGf7M/OxnP2vQda688kqHtujoaLVu3dru5x/wJMYAxgApMMeAxMREDRs2TOvWrdPq1as1duxYSdVVIw8fPqzBgwcbJvTv2bNHf/7zn7Vt2zYdPXq0ST8XNVl/dgcNGuTwWps2bXT55Zcb7hFTUFCgV199VevWrZPFYlFxcbFb+lPT7t27FRoaalit88orr1Tbtm21f/9+lZSU2M1OWl+vzRoImmUsI4gxqbi4OB04cEA//fSTw2sDBgyw+wGs/Ye2sLBQUvVUZ+2Brj6tW7c2bLd+klfzj6f1OvWtraz9iy1VD7hGvv76a40dO1bnz5/XkCFDNHr0aEVHRys0NFTff/+9Vq1a5fCHqy7WPv7tb39rUB9Pnz4tSU6nWZ31u6nOnTunEydOSJLdp2RPPPGE5s6dq/j4eA0fPlwJCQm2T0gXL17s8idmK1eu1D333KPIyEgNHTpUXbp0UYsWLRQaGqrNmzdry5YtDXq+mzdvdvjZGjRokEeDmLi4OMP2jRs36rbbblNISIhuuOEG3XzzzWrZsqVCQ0O1a9cuffbZZy4lMjqrXBMeHm6XrAl4EmMAY4AUuGPAvffeq3Xr1mnhwoW2IMYood9q+/btSklJUWVlpYYMGaJf/vKXatWqlUJDQ/Xdd99pzZo1Lv1c1GT9njv73hr9LJw4cUI33HCDDh06pKuvvlp33HGH2rZtq7CwMJ04cUJvvfVWo/tTU1FRkdq2bes0WI2Pj9eJEydUVFRkF8SEhYWpVatWDsdbl1XX/D32ZwQxJnXttddq06ZN2rhxo371q1+59F7rIHTjjTfq/fff90T37K6Tk5PToOS0mow+XZSkF154QSUlJVq5cqUGDx5s99pLL72kVatWNaqPX375ZYM+ibceb/0EsbZjx465dP2G2rZtm86fP6+4uDjbWuj8/HzNmzdPvXr10tq1ax2mtJcuXerydazVjtavX++wNOu///u/tWXLlgadZ/r06YbrrT3J2c/MrFmzVF5ernXr1jnM1jzzzDMuVa4B/AVjAGNAII8BN954oxISEvTFF1/o4MGDioiI0Nq1a3XRRRdpzJgxDsfPnj1bpaWlWrVqlQYOHGj32vPPP681a9Y0ui/W77mz763Rz8Jf//pXHTp0SP/7v/+rRx991O61rVu36q233mp0f2qKjo7WyZMnVVZWZhjI5OXl2Y4LROTEmNSkSZMUHh6uf/zjH9q/f79L77300kvVpk0b7dy506Ol9H7+859Lqv6FdZfs7Gy1bdvWYfCS5PSPa2hoqNNPyK19NJoKNnLppZeqRYsW2rNnj+F0a0P/wLuioqJCzz//vCQpNTXV1v7jjz+qsrJSQ4cOdfgDdeTIEf34448O5zL6tLSm7Oxs9ejRw2HwqqystJV/NJucnBx16tTJcLmZO382AW9iDGAMCOQxIDw8XJMmTbLlwSxatEjnz5/XxIkTDf9nPTs7W7GxsQ4BjNT074l12ZXReU6dOmWYU2ktH26dRWpIf6wbLLsyC9KnTx9VVlYannP37t06ceKELrvsMoelZIGCIMakunTpomnTpqmsrEy33Xab0/rtRn9kw8PD9cADDyg/P1+PPPKIzp4963BMQUGBdu/e3aQ+3n///YqIiFB6eroOHDjg8HpFRYWtNn1DJSYm6sSJEw5/NBYuXKjPP//c8D3t27fX8ePHVVJS4vDaXXfdpZiYGM2ePVtfffWVw+tVVVXatm2bbaBv1qyZUlNTVVxcbFfmUqpeN/vhhx+6dD/1ycvL07333qutW7cqMTFR//M//2N7zVoSc/v27XZ/9M6cOaPf//73On/+vMP5rMsQDh8+bHi9xMREZWdn2y1Rqaqq0syZM/Wvf/3LLffkbYmJicrLy7OtbbeaO3dug//HBfA3jAGMAYE+BlgT/BctWqSFCxcqJCTEIaHfKjExUcePH3coEvHOO+9ow4YNTerHmDFj1Lp1a3300Ud2lcyk6mppRhvJWr83tX++v/nmG7366quG16nve2PEOgs7Y8YMu2WZZWVlSk9PtzsmELGczMQeffRR2x+XUaNG6Wc/+5n69euntm3b6tSpUzp48KCtokrtTyceffRR7d27VwsXLtQ///lPXX/99erUqZOOHz+unJwcbd++Xffdd59hdZiG6t69u9544w1NmTJFAwYM0IgRI9StWzdVVFToyJEj2rFjh86dO6eDBw82+JwPPvigPv/8c40ePVopKSlq3bq1vvnmG23fvl3jxo3TP/7xD4f3DB06VB9++KFuvfVWDRw4UM2bN9fll1+u0aNHq23btlq4cKHuuusujRw5Utdff70uu+wyNWvWTEeOHFFmZqYOHz6sH3/80fbpz//93/9pw4YNmj9/vnbv3q2BAwfq6NGj+vjjjzVixAitXr3a5Wd16tQpW1JpRUWFTp06pX379mnHjh0qLy/Xz3/+c82fP1/t2rWzvScuLk633nqrli5dqsGDB2vo0KE6ffq01q9fr8jISF1xxRUOJTv79++vVq1aadmyZYqIiNDFF1+skJAQTZgwQYmJiZo8ebIefvhhDRkyRGPHjlV4eLh27Nih/fv368Ybb2zSlLyvPPjgg7rnnns0fPhwpaSkqGXLltq5c6d27typm2++WStXrvR1F4FGYQxgDAjkMSAxMVHDhw+3Lfm9/vrr1a1bN8NjJ0+erA0bNmjUqFFKSUlRdHS0du3apa+++kpjx47VihUrGt2P1q1b6+WXX9Z9992n0aNH2+0Ts2/fPg0YMMDhA7GJEydqzpw5mjZtmjZs2KCuXbvq3//+t9auXauxY8dq2bJlDtcZNmyYVq5cqYceesiWu9m2bVvdd999Tvt2++23a82aNfr44491zTXXaMyYMQoLC9OaNWv0n//8R8OGDdPvfve7Rt+7vyOIMbnHHntMt956q95++21t3LhRS5YsUXFxsVq1aqXk5GTde++9uv322x2W0oSHh2vhwoVaunSpFi1apM8++0xnzpxRu3bt1LlzZz388MO64447mtw/667Kr7/+ujZs2GD74xofH68RI0Zo3LhxLp1vxIgRev/99/XCCy/o448/VmhoqPr166eVK1fqxx9/NBzAZs6cqdDQUK1fv147duxQRUWF7rzzTo0ePVpS9R/GLVu2aM6cOfr888/11VdfKTw8XHFxcfr5z3+up556yi6ZtX379lq7dq3++Mc/as2aNfruu+90ySWX6IUXXlBiYmKjBrDTp0/bkiAjIiIUHR1tq+Qybtw4DRkyxDbVXNNrr72mLl26aNmyZVqwYIEuuugijR49Wk888YThpy9t2rTRokWL9Nxzz2nZsmU6c+aMpOr19YmJifr1r3+tiIgIzZ07V++9954iIyM1YMAAvf7661qxYoUpg5hx48bp3Xff1SuvvKKPPvpI4eHh6t+/v9asWaNdu3YRxMDUGAMYAwJ5DLj33nttQYy1lL2RUaNGafHixXrxxRe1bNkyhYWFqV+/fvrkk0+UlZXVpCBGkm699VbFxMTo+eef1/Lly9W8eXMNHDhQ69at0/PPP+8QxHTq1EmrV6/WjBkztHXrVn3++ee69NJL9fLLL2vQoEGGQcw999yjI0eOaOnSpXr99ddVXl6u5OTkOoOYkJAQ/eUvf9F1112nv//971q4cKGqqqrUrVs3PfPMM3rggQcMN7oMFGx2CQAAAMBUyIkBAAAAYCo+C2Lmz5+vgQMHqnPnzurcubN+8YtfaO3atXW+Z8+ePbrpppsUHx+vnj17atasWbYNvwAAqIlxBgACl88WynXs2FEzZsxQt27dVFlZqffee0+TJk3Sl19+qcsvv9zh+NOnT+uWW27RwIED9cUXXygrK0tTpkxRixYtbLsQAwBgxTgDAIHLr3JiunTpoqeeespwN9a//OUvevrpp3XgwAFbvevZs2fr7bff1t69e51ujAUAgBXjDAAEBr/IiamoqNDSpUtVXFys/v37Gx7z1VdfacCAAXYb9gwfPly5ubmyWCze6ioAwIQYZwAgsPi07tqePXs0cuRIlZaWqmXLlvr73/+u3r17Gx577NgxdezY0a4tNjbW9lqXLl083V0AgMkwzgBAYPLpTEz37t21adMmrVu3Tr/97W/14IMPau/evU6Prz2Vb022ZIofAGCEcQYAApNPg5iIiAh17dpVV111lZ566ildccUVeuONNwyP7dChg44dO2bXdvz4cUkXPilzt6ysLI+c1wyC9d657+ASKPc9ZnW+Yt454vDv5tX5hscHyn03hDfHmWB6rnXhOVzAs6jGc6jGc7jAHc/CL3JirCorK1VWVmb4Wv/+/bVt2zaVlpba2tavX6+EhAQlJSV5q4sA4HcSWoQZtsc7aQ9mjDMAEBh8FsQ8/fTT2rp1qywWi/bs2aMZM2Zo8+bNSk1NlSTNmDFDY8eOtR1/2223KSoqSpMnT9bevXu1YsUKvfLKK5o8eTLT/ACCWnrfaCVH2wcsydFhSu8b7aMe+QfGGQAIXD5L7D969Kjuv/9+HTt2TK1bt1bv3r310Ucfafjw4ZKkvLw85eTk2I5v06aNPv74Yz3yyCMaOnSoYmJiNGXKFE2dOtVXtwAAkiRLUbkydhUp92yFElpUBw9J0c28dv2k6GZaPqq9MnYVKe9sheJ90Ad/xDgDAIHLZ0HM3LlzXX69d+/eWr16tae6BAAusxSVK2VtgXKKKmxtmfllWj6qvdcDmflD2nntembAOAMAgcunJZYBwOwydhXZBTCSlFNUoYxdRW4LKmrO9LRuFqKqKqnofJVPZn0AAOYXYrEoMiNDobm5qkxIUGl6uqpMlvtHEAMATZB7tsKwPc9Ju6uMZnpq8sWsDwDAvEIsFrVMSVFYjeW0YZmZKl6+3FSBjF9VJwMAs6mvMpilqFxpGwo1ZnW+0jYUylJU7tL5jWZ6arLO+gAA0BCRGRl2AYwkheXkKDIjw0c9ahxmYgCgCdL7Riszv8wu0LBWBnNHvoyzmZ6ack67FhgBAIJXaG6ucXtenpd70jQEMQDQBHVVBkvbUOhSvoxRlTNnMz01HSutctv9AAACW2VCgnF7fLyXe9I0BDEA0ETOKoO5ki/jbNZmzqA2DjM9tcVFhfq8zDMAwBxK09MVlplpt6SsIjlZpenpPuyV6whiAMBD6suXqclZlbO/HiixzfR8+VOp8g1mXWIjQ/2izDMAwP9VJSWpePny6upkeXmqjI+nOhkA4IK68mVqq2vWxjrTYzRbkxwdpqoqebzMMwAgcFQlJalk/nxfd6NJqE4GAB5izZdJ7RqlwfERSu0a5XR2pCGzNs7OV3TeOCfGXWWeAQDwN8zEAIAHOcuXqa2hszZG53Nl2RoAAIGAmRgA8AOuzNrUlt43WsnR9gGLs2VrAAAEAmZiAMBPNHTWxuh9zso8AwAQiAhiACAANDYAAgDAjFhOBgAAAMBUCGIAAAAAmArLyQDAgKWoXBm7ipR7tkIJbsox8cQ5AQDwlRCLpXrTzNxcVSYkeHXTTIIYAKjFaFPJzPyyBlcL89Y5AQDwlRCLRS1TUhSWk2NrC8vMVPHy5V4JZFhOBgC1ZOwqsgs2JCmnqEIZu4r86pwAAPhKZEaGXQAjSWE5OYrMyPDK9QliAKCWXCc73ec5affVOQEA8JXQ3Fzj9rw871zfK1cBABNJcLLTfbyTdl+dEwAAX6lMSDBuj4/3yvUJYgCglvS+0UqOtg8ukqOrE/H96ZwAAPhKaXq6KpKT7doqkpNVmp7uleuT2A8AtSRFN9PyUe2VsatIeWcrFN+ASmL1VR5rzDkBAPBXVUlJKl6+vLo6WV6eKuPjqU4GAL6WFN1M84e0a9CxDa085so5AQDwd1VJSSqZP98n12Y5GQCoOhBJ21CoMavzlbahUJai8ga/l8pjAAB4FzMxAIJeU/dwofIYAADexUwMgKDX1JkUKo8BAOBdzMQACEqWonI9ub+Zzvw7X/tPGi8da+hMSnrfaGXml9kFQlQeAwDAcwhiAPiF+qp7ufta1cvHmkkqc3pcdLMQpW0orLdPVB4DAMC7CGIA+FxTc1JcZbR8rLaLW4Zqd0GZDp+talCfqDwGAID3kBMDwOe8Xd3LWSK+VWKrMF3RtpldAOPpPgEAgIYjiAHgc96u7uUsEd8qqXAqXikAACAASURBVFWYis5XGb5GxTEAAHyP5WQAfM5ZUNHQnBRXGSXi11RXVTEqjgEA4HsEMQB8ziiocDUnxRXWRPyHPj+ir043U2mNWKZmVTEqjgEA4J8IYgD4nFF1rzNllVp9+JzdcdacFHck0CdFN9OLvcsVEd/JaVUxKo4BAOCfCGIA+IXa1b3GrM43PM7dOSl1VRWj4hgAAP6JxH4AfslZngw5KQAAgJkYAH7JKE+msTkp3txIEwAAeJ7PZmJeeuklDR06VJ07d1a3bt00YcIE7d27t873WCwWxcTEOPxbt26dl3oNwFuseTKpXaM0OD5CqV2jGpXUb91Ic0l2iTbnlWlJdolS1hbIUlTuoZ7DXzDOAEDg8tlMzObNm/Xb3/5Wffv2VVVVlZ599lmlpKRox44datu2bZ3vXbp0qS6//HLb1/UdD8Cc3JGTUtdGmo91bNKp4ecYZwAgcPksiFm2bJnd1/PmzVNiYqK2b9+u0aNH1/nedu3aKS4uzpPdA+AHGroMzOg4qTqAWXuo1PDcbFoZ+BhnACBw+U1OzJkzZ1RZWamYmJh6j/3Vr36l0tJSdevWTZMnT9a4ceO80EMA3mRdBlZzFsVonxij47YdPSdVVdntMVMbBQKCD+MMAASOkJMnTzof5b3o3nvv1X/+8x99+eWXCgsz/p+LgoICLV68WNdee63Cw8O1atUqvfjii5o7d64mTJjg9NxZWVme6jYAD3lyfzOtyXecdbm+3Xm92Kus3uPqcnFkpeb0PqdOUX7x5880unfv7usuNAnjDAD4N1fGGb8IYp544gktW7ZMa9asUZcuXVx67x/+8Adt27ZNW7dudXu/srKyTD9oN1aw3jv37T/GrM7X5rwyh/bIUGnH+A622Rhnxxlp0yxEIztH2pal+eN9e0Mw3rc3xplgfK5GeA4X8Cyq8Ryq8RwucMez8Pk+MdOnT9fSpUu1YsUKlwcWSerXr5+ys7Pd3zEAPuVsn5jSyupcl/qOMzKyc6TmD2lHeeUgwzgDwFNCLBZFpaWp5ZgxikpLU4jF4usuBQ2f5sRMmzZNy5Yt0yeffKJLL720Uef4/vvvSb4ETMxZ8n5632ittJSo1CD/vmZSvtF+MkYau8cMzI1xBoCnhFgsapmSorCcHFtbWGamipcvV1VSkg97Fhx8FsQ88sgj+uCDD/T3v/9dMTExOnr0qCSpZcuWatWqlSRpxowZ2rlzp1asWCFJWrx4sZo1a6Y+ffooNDRUa9as0YIFC/T000/76jYANEF9yftDE5pr9eFzDu+rmZRv3U8mY1eRvvypVPmljitkE1uFNWqPGZgb4wwAT4rMyLALYCQpLCdHkRkZKpk/30e9Ch4+C2IWLFggSQ4VX6ZNm6bp06dLkvLy8pRT64fjhRde0KFDhxQWFqZu3bppzpw5dSZbAvBfde3hMn9IO828to3+VSvIMZpRse4nYxQUtQwP0dzr2hDABCHGGQCeFJqba9yel+flngQnnwUxJ0+erPeYuXPn2n09ceJETZw40VNdAuBluU72arEuF6s5y5J3tkLxdewVYz1+zqA2un1doYrPV7cVn6/S1C2ntHxUOIFMkGGcAeBJlQkJxu3x8V7uSXDym31iAPjOkZIQPb+hsN5NJd3NWVJ+7eVi84e0a/A5/3qgxBbAWNWc3QEAwB1K09MVlplpt6SsIjlZpenpPuxV8CCIAYKcpahcU/c01+HSElub0aaSnmCUlN/UBPz6ZncAAHCHqqQkFS9frsiMDIXm5akyPl6l6ekk9XsJQQwQ5DJ2FelwqX21dW/NXLi6XKwhGjK7AwCAO1QlJZHE7yM+3ycGgG/5eubCWk45vkWYcs9WB0+WovJGny+9b7SSo+0DFsorAwAQWJiJAYKcr2cu6iuz7CpPzO4AAAD/QhADBLn0vtHa9lOx3ZIyd89cONvQUqq/zHJjuFoMAAAAmAtBDBDkkqKbaU7vc1p0op1HZi7qm2nx9XI2AABgPgQxANQpqkrz+3hm5qK+mRZfL2cDAADmQ2I/AI+qb6aFRHwAAOAqZmIAeFR9My0k4gMAAFcRxAAwVFcyvisasqElifgAAMAVBDEAHLiz7DEzLQAAwN0IYgA4cHfZY2ZaAACAO5HYD8ABZY8BAIA/I4gB4ICyxwAAdwuxWBSVlqaWY8YoKi1NIRaLr7sEE2M5GQAHDUnGBwCgoUIsFrVMSVFYTo6tLSwzU8XLl6sqKcmHPYNZMRMDwIE1GT+1a5QGx0cotWtUo5L6AQCQpMiMDLsARpLCcnIUmZHh0nmYzYEVMzEADJGMDwBwl9DcXOP2vLwGn4PZHNTETAxgQpaicqVtKNSY1flK21AoS1G5r7sEAIBTlQkJxu3x8Q0+h7tmcxAYmIkBTMade7gAAOANpenpCsvMtAtCKpKTVZqe3uBzuGM2B4GDmRjAZOraw8XTmAECADRGVVKSipcvV1lqqs4PHqyy1FSXl4G5YzYHgYOZGMBkfLWHCzNAAICmqEpKUsn8+Y1+vztmcxA4mIkBTMZXe7j4cgYIAAB3zOYgcDATA5iMO/ZwsRSVK2NXkXLPViihRZgmtQ1R93re46sZIAAArJo6m4PAQRADmIx1D5eMXUXKO1uh+BbVAUxDl3QZLQvbFtlcnyaX13kOX80AAQC8K8RiUWRGhkJzc1WZkKDS9HRmO+B3CGIAE2rKHi5Gy8IOl4YqY1dRned0xwwQAMC/sRcLzIKcGCDINHZZmHUGKLVrlAbHRyi1axRJ/QAQYNiLBWbBTAwQZJwtC7OcqZClqO4lZU2ZAQIA+D/2YoFZMBMDBJn0vtFKjnYMZA6eqVDK2gL2fgGAIMZeLDALghggyFiXhSW2cgxkKJkMAMGtND1dFcnJdm3sxQJ/RBADBKGk6GaGQYxEyWQACGbsxQKzICcGCFKUTAYAGGEvFpgBMzFAkDLKjaFkMgAAMAOCGCBI1SyZ3K9NBSWTAQCAabCcDAgglqJyZewqUu7ZCiW0qJ5VaUjJ5KysAnXvnujFngIAADQeQQwQICxF5UpZW6CcoguJ+Zn5ZU5nV2oGPK0qmmlWfN17xAAAAkOIxaLIjAyF5uaqMiFBpenpJO7DdAhigACRsavILoCRLpRMrr1BpWPA00z71xawnAwAAlyIxaKWKSkKy8mxtYVlZlKBDKZDTgzg5yxF5UrbUKgxq/OVtqHQ6WaUuU5KIxuVTK4r4AEABK7IjAy7AEaSwnJyFJmR4aMeAY3jsyDmpZde0tChQ9W5c2d169ZNEyZM0N69e+t93549e3TTTTcpPj5ePXv21KxZs1RVVeWFHgPeZ50xWZJdos15ZVqSXaKUtQWGgYwrJZNdCXgAs2KcARyF5uYat+flebknQNP4LIjZvHmzfvvb32rt2rVasWKFwsPDlZKSohMnTjh9z+nTp3XLLbeoQ4cO+uKLLzRz5ky99tprmjNnjhd7DniPKzMmrpRMZo8YBAPGGcBRZUKCcXt8vNuvFWKxKCotTZc+8ICi0tIUYrG4/RoIXj7LiVm2bJnd1/PmzVNiYqK2b9+u0aNHG75nyZIlKikp0dy5cxUVFaVevXrpwIEDeuONNzR16lSFhIR4o+tAg7laLaw2V2ZMrCWTM3YVKe9sheLruF5632hl5pfZBUjsEYNAwzgDOCpNT1dYZqbdkrKK5GSVpqe79To1c28iJGnnTnJv4FZ+kxNz5swZVVZWKiYmxukxX331lQYMGKCoqChb2/Dhw5WbmysL0T38jCtLwZxxdcbEWjJ55ehYzR/SzmnAVHOPmMHxEboxtpykfgQ8xhlAqkpKUvHy5SpLTdX5wYNVlprqkcCC3Bt4mt9UJ3v88cd1xRVXqH///k6POXbsmDp27GjXFhsba3utS5cuhu/LyspqdL+a8l6zC9Z7d9d9P7m/mXKK7IOCnKIKTdtwRM/0aFggM6ltiLZFNtfh0gufN1wcWalJbQuVlVXQ5D4+VuPXqSzvR2UF4ZJofs4brnv37h7oifd4Y5wJ1p+n2ngOF/jts3jssQv/XVYmubmfl2ZnV8/A1FKane2/z8QLgvneazN6Fq6MM34RxDzxxBPavn271qxZo7Cwutfk157KtyZb1jXF39iBNysry/SDdmMF6727877P/DtfUplDe3FYS3XvHtugc3SX9GlyeYOWiDUF3+/gEoz37Y1xJhifqxGewwXB/Cwiu3aVdu40bA/WZxLMPw+1ueNZ+DyImT59upYtW6aVK1c6/YTLqkOHDjp27Jhd2/HjxyVd+KQM8BfuSp63LhED0DiMM0A1b25y6a3cGwQvn+bETJs2TR999JFWrFihSy+9tN7j+/fvr23btqm0tNTWtn79eiUkJCiJJDH4GVeqhQHwDMYZoJo10T5iyRKFb96siCVL1DIlxWMVw2rm3pzu189juTcIXj4LYh555BEtXrxYCxYsUExMjI4ePaqjR4/qzJkztmNmzJihsWPH2r6+7bbbFBUVpcmTJ2vv3r1asWKFXnnlFU2ePJmKMfA7tZPnU7tGkTwPeBHjDHCBLxLtq5KSVDJ/vg68+aZK5s8ngIFb+Ww52YIFCyRJ48aNs2ufNm2apk+fLknKy8tTTo1fuDZt2ujjjz/WI488oqFDhyomJkZTpkzR1KlTvddxwAUsBQN8h3EGuIBNLhFofBbEnDx5st5j5s6d69DWu3dvrV692hNdAhqsqfu/uOscAJxjnAEu8OYml4A3+DyxHzAb6/4vNTeKzMwvc2mpmDvOAQBAQxkl2le1bKlz997ru04BTeA3m10CZpGxq8gu+JCq93/J2FXk1XMAAMwrxGJRVFqaWo4Zo6i0NI8l2FtVJSXp7Jw5qmzZ8kIfiovVYupUj18b8ARmYgAX5Z6tMGzPc9LuqXMAAMzJWims5qxIWGamx6t3Nf/rXxVaXGzXZk3uL5k/32PXBTyBmRjARe7Y/yU63LjKUSsn7QCAwOGLSmESyf0ILAQxgIvcsf+Ls0qtVHAFgMDnq2CC5H4EEoIYwEXu2P/ldHmVYXuRk3YAQODwVTBRmp6uiuRku7aK5GSVpqd79LqAJ5ATAzRCU/d/cceSNACAORlVCvNGMFGVlKTi5csVmZGh0Lw8VcbHqzQ9nU0oYUoEMYAPpPeNVmZ+mV2FMleXpAEAzMkWTEyfrvCvv5YkVVx2mdeuTRI/AgFBDOAD1iVpGbuKlHe2QvFsdgkAQSds3z6F5udLkiJWr1bYv/7l8QplQKAgiIFpBNoO901dkgYAMK+6KpQxUwLUjyAGpsAO9wCAQEK5Y6BpqE4GU2CHewBAIKHcMdA0BDEwBTPscG8pKlfahkKNWZ2vtA2FshSV+7pLAAA/RbljoGlYTga/VDv/pXUz410gG1qS2NP5NCx3AwC4gnLHQNMQxMDvGAUEF7cI0cUtQ3W4uNLW1tCSxN4IMOpa7kbyPgDACOWOgcYjiIHfMQoIDp+t0uiLIzQgLrTeksS1Z13OlFV6PMAww3I3AACAQEEQA7/jLCA4fq5SrSJCVVXHe41mXSKdrDhzZ4CR4GRZW0OXuwEAAKDhCGLgd5wFBHtPnNfX+ReS5Y2WhBnN4pQ6iVXcGWCk941WZn6Z3bUbutwNAAAArqE6GfxOet9oJUfbBxgtw6Xi8/ZzMEYllp3N4tT+QXd3gJEU3UzLR7VXatcoDY6PUGrXKJL6AQAAPISZGPgda0CQsavIlv+SfbpcO4+fdzi29pIwZ7M4laoOhHrGhCu5dTO3Vyez9pskfgAIPiEWS3WVsdxcVSYkUGUM8AKCGPil2gFB2oZCwyCm9pIwo2VdVsXnpeTWBBoAAPcJsVjUMiVFYTk5trawzEwVL1/uw14BgY/lZDAFoyVmRkvCrLM4sZHG+8pQLQwA4E6RGRl2AYwkheXkKDIjw0c9AoIDQQxMwZWck6ToZrqhY6TheagWBgBwp9DcXOP2vDwv9wQILiwng2m4knNCtTAAgDdUJiQYt8fHe7knQHBhJgYBiWphAABvKE1PV0Vysl1bRXKyStPTfdQjIDgwE4OARbUwAICnVSUlqXj58urqZHl5qoyPv1CdLCvL190DAhZBDAAAQBNUJSWpZP58X3cDCCosJwMAAAElxGJRVFqaWo4Zo6i0NIVYLL7uEgA3YyYGAAAEjLr2bWEDSiBwMBMDj7AUlSttQ6HGrM5X2oZCWYrKfd0lAEAQYN8WIDgwExPELEXlythVpNyzFUpoUV1+2B3VuyxF5UpZW2BX3jgzv8xWHazmdaPDQxQSIp0ur3JrHwAAwcld+7aEWCzVyfq5uapMSLiQrA/ALxDEBKn6Ao2myNhVZHdeScopqlDGriKl9412uG5N7uoDACA4uWPfFpakAf6P5WRBqq5Ao6lyzxoHKHlnKwyv64k+AACCkzv2bWFJGuD/mIkJUnUFGk2V0CLMsD2+RZjT67q7DwCA4FTnvi0N5K4laQA8hyAmSNUVaDRVet9oZeaX2c24JEdX57s0ZJalrj54Ko8HABA4mrpvizuWpAHwLJaTBan0vtFKjrYPFqyBRlMlRTfT8lHtldo1SoPjI5TaNcqW52J03Yb2wZrHsyS7RJvzyrQku0QpawuofAYAsNPUfWLcsSQNgGcxExOkrIFGxq4i5Z2tULybZzWSoptp/pB29V631f+vTlZUXmXXB6MZl7ryeIyuBQAIPu5IynfHkjQAnkUQE8ScBRqe4MoyMGeV09pHGk8ckkMDALCqKynflSVmTV2SBsCzXFpO9s9//lOVlZVuu/iWLVt0xx13qGfPnoqJidGiRYvqPN5isSgmJsbh37p169zWJ7ifq8vAnM24HCsx/tlzRx4PAP/AOIOmIikfCA4uBTETJkzQZZddpunTp+vbb79t8sWLi4vVq1cvzZw5U1FRUQ1+39KlS7V//37bv+uvv77JfYHnuFrO2VkFsw6RIR7L4wHgHxhn0FQk5QPBwaUg5v3339fgwYO1cOFCDRs2TNdcc41efvllHT58uFEXHzlypP7v//5P48aNU2how7vSrl07xcXF2f5FREQ06vrwDlfLOTurnJbc2nnBAACBgXEGTUVSPhAcXApiRo0apb/85S/av3+/XnvtNSUkJCgjI0NXXnmlbr75Zi1atEhFRZ7fqPBXv/qVLrnkEo0aNUr/+Mc/PH69YGYpKlfahkKNWZ2vtA2FjaoE5mo557oqp1nzeFaOjtX8Ie0IYIAAwziDprIm5Zelpur84MEqS011KakfgDmEnDx5sqopJ8jLy9OSJUv0wQcfaO/evYqMjNRNN92kO++8U8OHD2/weTp16qTnn39ekyZNcnpMQUGBFi9erGuvvVbh4eFatWqVXnzxRc2dO1cTJkxw+r6srCyX7ilYHCkJ0ZsHw5V/LlSxzSv1QOJ5dYqqsnt96p7mOlx6Ida9OLJSc3qfszuuIddx9Ty2vpWFKjbCsW8AvK979+4+uS7jDAAEB1fGmSZXJysvL1dZWZnKyspUVVWl6Ohobdu2TUuXLlXPnj311ltv6fLLL2/qZSRJ7du310MPPWT7+qqrrlJhYaFeffXVOgeXxg68WVlZPhu0Pc1SVK6H7SqAhWl/aaRteVZWVpYWnWinw6Uldu87XBqqRSfaaX6fhlc16y7p0+Ryl8o5d5d0Qx/X76upAvl7XhfuO7iY7b7NMs6Y7bl6Cs/hAp5FNZ5DNZ7DBe54Fo3a7PLUqVN69913ddNNN+lnP/uZZs+erV69eun999/X3r179cMPP+i9995TcXGx3WDgCf369VN2drZHr+FN7li+1RDOku1HfFJ93SMlIS7nstSFZWAAXME4AwCoi0szMZ9++qk++OAD/fOf/9S5c+d09dVXa/bs2Ro/frxiYmLsjr3xxht17Ngx/eEPf3Brh2v7/vvvFRcX59FreIuz/VE8kbzuLEDJL63SkuwSbYtsrisuCjE8xp0ljV3ZPwZA4GOcAQA0hEtBzF133aVOnTppypQpuvPOO3XJJZfUeXzv3r2Vmprq9PUzZ87YPt2qrKzU4cOHtXv3brVt21adO3fWjBkztHPnTq1YsUKStHjxYjVr1kx9+vRRaGio1qxZowULFujpp5925Tb8ljd3pHeWbG91uDRUfUKqE+pr9smdJY29GbQBMAfGGQBAQ7gUxHz88ccaMmSIQkKMP6GvrV+/furXr5/T17/55hvdfPPNtq+fe+45Pffcc7rzzjs1d+5c5eXlKafWrrsvvPCCDh06pLCwMHXr1k1z5sypc52ymbhz+VZ90vtGKzO/zCFoqqmovErLR7V3KZfFFd4M2gCYA+MMAKAhXApibrjhBrdefPDgwTp58qTT1+fOnWv39cSJEzVx4kS39sGfuFqKuCmSopvZApQvfzqn/FLHHbLjW4TZclk8wZtBGwBzYJwBADREoxL74Rl17Y/iCdYAZd2Yixyue3Fkpceua+XNoA0AAACBo8klluE+NWdHPLF8y5XrTmpb6PHrGi1p82TQBgAAgMBAEONnPLl8qyajqmA1r5uVVeDxPvgqaAMAAIC5EcQEEWvgklN0XvtOnFfx+Srba9aqYFJ1wn328ebq+lOhx4MKbwVtAAAACBwEMUHCqJxxTTlFFXp8+yn969T5/39MmHaeLqHkMQAEmRCLRZEZGQrNzVVlQoJK09NVlZTk624BgB2CmCBhVM64tszj5Q5Vyih5DADBI8RiUcuUFIXVKDsdlpmp4uXLCWQA+BWqkwUJZ+WM7VUZtlLyGACCQ2RGhl0AI0lhOTmKzMjwUY8AwBhBjIlYisqVtqFQY1bnK21DoSxF5Q1+r7NyxlbJ0WH6eWyE4WuUPAaA4BCam2vcnpfn5Z4AQN1YTmYSRjkttZPxa1Yaq53DYlTOuGW41DMmXMmtm9nKGu87WUDJYwAIUpUJCcbt8fFe7gkA1I0gxiSMclock/GrGSXjN7ScsfWY7IIz6tq+FSWPAcBkmpKYX5qerrDMTLslZRXJySpNT/dUdwGgUQhiTMJZTouzZPzpO06pZbNQh9mZ+hL0rcdkZRWoe/dEt/UfAOB5TU3Mr0pKUvHy5dVBUF6eKuPjqU4GwC8RxJiE85wW42T8L46cU83YhlLJABD46krML5k/v0HnqEpKavCxAOArJPZ7WFOS8WtK7xut5Gj7QKauZPxakzO2UskN7euT+5s1uq8AAN8gMR9AsGAmxoPqSsZ3dUbEWU6L5JiMHxkmlRqsPqurVLJjX5tp/9oCZm8AwERIzAcQLAhiPMhZMn5jN490ltNSO7g5U1ap1YfPORxXV6lkd/cVAOB9JOYDCBYEMR7kLBm/sZtHWorKDUsp1w5uLEXl+tda10olu7uvAADvIzEfQLAgiPEgZ8n4jdk80pWlaQ0tp+ypvgIAfIfEfADBgCDGg4w2mGzs5pGuLvdqSDllT/UVAAAA8CSCGA9qzIyIM55e7lW7ry0rijVrSAeS+gEAAOB3KLHsYUnRzZTeN1rxLcKUe7Z65qQxpYu9sdzLOnuzcnSsnulRTgADAAAAv8RMTBM5S7av+bo7yiyz3AsAAACoRhDTBA0JUJzlsty8pkCJrcIMAx8j7lyaBgAwlxCLpbriWG6uKhMSqDgGIOgRxDRBQ5LtneWyHDxToYNnql9r6MyMq8n6AADzC7FY1DIlxW7vl7DMTBUvX04gAyBokRPTBA1JtneWy1KTNfCpi6WoXGkbCjVmdb7SNhQ2Kq8GAGA+kRkZdgGMJIXl5CgyI8NHPQIA32Mmpgkakmyf3jda2/JKdfhsVZ3nqqvKmLvyagAA5hOam2vcnpfn5Z4AgP9gJsZFNWdEzpRV6uKW9o+wdrJ9UnQz9WkfUe9566oyVteyNQCAeYVYLIpKS1PLMWMUlZamEIvF4ZjKhATD91bGx3u6ewDgt5iJcYHRjMjFLUJ0U+fmKiqvcppsf7q87lmY+qqM1bdsrb4KaQAA/9PQXJfS9HSFZWbaHVeRnKzS9HSv9hcA/AlBjAuMZkQOn63SgPhQLR7hPOHe2bKz2MhQ3dCxeb1BR13L1lhqBgDmVFeuS8n8+ba2qqQkFS9fXl2dLC9PlfHxVCcDEPQIYlzQkER+I872eGlooFHXHjENqZAGAPA/ruS6VCUl2QU2ABDsCGJc0JBEfiNN3eOlrvc3NrACAPgWuS4A0HgEMS4wmhFpGS7de2lUve9t6h4vzt7f2MAKAOBb5LoAQONRncwFSdHNNGdQG7UMD7G1FZ+Xpm451aR9W5qyB0x632glR9sHLPUVCgAA+J4116UsNVXnBw9WWWoqG1gCQAMxE1OHIyUhen5DoV3Vr78eKFHxeftqY03JQWlqYn5Tl6oBAHyHXBcAaByCGCcsReWauqe5DpeW2Noy88vUrnmI4fGNzUFxR2J+U5eqAQAAAGZCEONExq4iHS61X22XU1Shiir35qCQmA8AAAC4hpwYJ5wFF3FRoW7NQSExHwAAAHANQYwTzoKLLtHhWj6qvVK7RmlwfIRSu0Y1aWNJEvMBAAAA1/g0iNmyZYvuuOMO9ezZUzExMVq0aFG979mzZ49uuukmxcfHq2fPnpo1a5aqqqrqfZ+r0vtG6+LISrs2a3BhzUFZOTpW84e0a1ISvTUx311BEQDgAn8eZwAAjefTnJji4mL16tVLd955px544IF6jz99+rRuueUWDRw4UF988YWysrI0ZcoUtWjRQg899JBb+5YU3Uxzep/TohPtXKr6ZSkqV8auIruKZvW9h8R8APAMfx5nAACN59MgZuTIkRo5cqQkafLkyfUev2TJEpWUlGju3LmKiopSr169dODAAb3xxhuaOnWqQkKMK4c1VqeoKs3v0/DgoqnlkgEA7uXv4wwAoHFMlRPz1VdfacCAAYqKirK1DR8+XLm5ubJYLD7sWbW6yiUDAPyfv48zAIBqpiqxfOzYMXXs2NGuLTY21vZaly5dDN+XlZXV6Gu68t7s480lORYEyC44o6ys2uXIegAAEo1JREFUgkb3wVea8tzMjPsOLtx3w3Xv3t0DPfEvTR1ngvXnqTaewwU8i2o8h2o8hwuMnoUr44ypghhJDlP51mTLuqb4GzvwZmVlufTerj8VaufpEsf29q3UvXtio/rgK67ee6DgvoML9w0jjR1neK7VeA4X8Cyq8Ryq8RwucMezMNVysg4dOujYsWN2bcePH5d04ZMyX6JcMgCYm7+PMwCAaqYKYvr3769t27aptLTU1rZ+/XolJCQoKSnJhz2rRrlkADA3fx9nAADVfBrEnDlzRrt379bu3btVWVmpw4cPa/fu3Tp06JAkacaMGRo7dqzt+Ntuu01RUVGaPHmy9u7dqxUrVuiVV17R5MmT/aZijDv3kAEANE0gjjMAAB8HMd98842uv/56XX/99SopKdFzzz2n66+/Xs8++6wkKS8vTzk5Obbj27Rpo48//li5ubkaOnSoHn30UU2ZMkVTp0711S0AAPwY4wwABCafJvYPHjxYJ0+edPr63LlzHdp69+6t1atXe7JbAIAAwTgDAIHJVDkxAAAAAEAQAwAAAMBUCGIAAAAAmApBDAAAAABTIYgBAAAAYCoEMQAAAABMhSAGAAAAgKkQxAAAAAAwFYIYAAAAAKZCEAMAAADAVAhiAAAAAJgKQQwAAAAAUyGIAQAAAGAqBDEAAAAATIUgBgAAAICpEMQAAAAAMBWCGAAAAACmQhADAAAAwFQIYgAAAACYCkEMAAAAAFMhiAEAAABgKgQxAAAAAEyFIAYAAACAqRDEAAAAADAVghgAAAAApkIQAwAAAMBUCGIAAAAAmApBDAAAAABTIYgBAAAAYCoEMQAAAABMhSAGAAAAgKkQxAAAAAAwFYIYAAAAAKZCEAMAAADAVAhiAAAAAJgKQQwAAAAAUyGIAQAAAGAqBDEAAAAATMXnQcyCBQvUp08fxcXFaciQIdq6davTYzdt2qSYmBiHfwcOHPBijwEAZsI4AwCBJ9yXF1+2bJkef/xxvfjii7r22mu1YMECpaamavv27ercubPT923fvl1t27a1fX3RRRd5o7sAAJNhnAGAwOTTmZjXX39dEydO1D333KMePXpo9uzZiouL09tvv13n+2JjYxUXF2f7FxYW5qUeAwDMhHEGAAKTz4KYsrIyffvttxo2bJhd+7Bhw7Rjx44633vDDTeoR48eGjt2rDZu3OjJbgIATIpxBgACl8+WkxUUFKiiokKxsbF27bGxsTp27Jjhe+Lj4/XSSy+pb9++Kisr0wcffKBx48bpk08+0aBBg5xeKysrq9H9bMp7zS5Y7537Di7cd8N1797dAz3xHF+MM8H681Qbz+ECnkU1nkM1nsMFRs/ClXHGpzkxkhQSEmL3dVVVlUObVffu3e1urn///jp48KBee+21OgeXxg68WVlZphu03SVY7537Di7cd3Dw1jgTbM/VGZ7DBTyLajyHajyHC9zxLHy2nKx9+/YKCwtz+DTs+PHjDp+a1aVfv37Kzs52d/cAACbHOAMAgctnQUxERIR+9rOfaf369Xbt69ev1zXXXNPg83z//feKi4tzd/cAACbHOAMAgcuny8mmTJmi3/3ud+rXr5+uueYavf3228rLy9Ovf/1rSdLvfvc7SdK8efMkSW+88YYSExPVs2dPlZWV6cMPP9Snn36qhQsX+uweAAD+i3EGAAKTT4OY8ePHq7CwULNnz9bRo0fVs2dPffjhh0pMTJQkHT582O748vJyPfnkk8rNzVVkZKTt+JEjR/qi+wAAP8c4AwCBKeTkyZNVvu6EvwrmBKxgvXfuO7hw33Annms1nsMFPItqPIdqPIcLTJ3YDwAAAACNQRADAAAAwFQIYgAAAACYCkEMAAAAAFMhiAEAAABgKgQxAAAAAEyFIAYAAACAqRDEAAAAADAVghgAAAAApkIQAwAAAMBUCGIAAAAAmApBDAAAAABTIYgBAAAAYCoEMQAAAABMhSAGAAAAgKkQxAAAAAAwFYIYAAAAAKZCEAMAAADAVAhiAAAAAJgKQQwAAAAAUyGIAQAAAGAqBDEAAAAATIUgBgAAAICpEMQAAAAAMBWCGAAAAACmQhADAAAAwFQIYgAAAACYCkEMAAAAAFMhiAEAAABgKgQxAAAAAEyFIAYAAACAqRDEAAAAADAVghgAAAAApkIQAwAAAMBUCGIAAAAAmApBDAAAAABTIYgBAAAAYCo+D2IWLFigPn36KC4uTkOGDNHWrVvrPH7z5s0aMmSI4uLidOWVV+rtt9/2Uk8BAGbEOAMAgcenQcyyZcv0+OOP6w9/+IM2btyo/v37KzU1VYcOHTI8/scff9Ttt9+u/v37a+PGjfqf//kfPfbYY/rHP/7h5Z4DAMyAcQYAApNPg5jXX39dEydO1D333KMePXpo9uzZiouLc/qp1zvvvKP4+HjNnj1bPXr00D333KM777xTc+bM8XLPAQBmwDgDAIHJZ0FMWVmZvv32Ww0bNsyufdiwYdqxY4fhe7766iuH44cPH65vvvlG5eXlbu9j9+7d3X5OswjWe+e+gwv3Hdi8Pc4Ey3OtD8/hAp5FNZ5DNZ7DBe54Fj4LYgoKClRRUaHY2Fi79tjYWB07dszwPceOHTM8/vz58yooKPBYXwEA5sM4AwCBy+eJ/SEhIXZfV1VVObTVd7xROwAAEuMMAAQinwUx7du3V1hYmMOnYcePH3f4FMyqQ4cOhseHh/+/9u4/pqr6j+P4y/lj2kpIBkJqaZQDZC3u2jUGZAm2pTWn/xTTP5oWuOiHbUHSEmXlkK65lEoSo1axNpyZFPZHCaZ5L6sNLaXdWqxfWlz6MdZgmLLu94++3OGFynu59x7O/TwfG39w7mH3/d7dzov3PT8+UzRr1qyo1QoAsB9yBgDil2VDzLRp03TzzTervb39ku3t7e1avHjxmH/jdDp19OjRUfvn5ORo6tSp0SoVAGBD5AwAxK/JmzZt2mrVm1911VWqqalRamqqpk+fLpfLJbfbrRdffFEJCQkqLS3V+++/r3vuuUeStGDBAr3wwgv65ZdfNG/ePB0+fFjPP/+8nn32WWVkZFjVBgBggiJnACA+WXpPzOrVq1VTUyOXy6WCggJ1dHSoublZ1157rSTp7NmzOnv2bGD/+fPnq7m5WW63WwUFBdqxY4dqa2u1cuXKsN7f1AXQQum7paVFq1atUnp6uubOnavCwkIdPnw4htVGVqif+TCPx6OkpCTl5uZGucLoCLXvCxcuaNu2bbrpppuUkpKi7Oxs1dfXx6jayAm17/379ys/P19paWlauHChSkpK5PP5YlRtZJw4cUL33XefMjMzlZiYqKampv/8m66uLi1fvlypqanKzMxUbW1t4D4Qu4tkzpiaGcFMzpCRTM2TsZiaMcFMzJxgscwgy2/sf+CBB3T69Gn19vbq448/Vl5eXuC11tZWtba2XrJ/fn6+jh07pt7eXn3xxRdat25dWO9r6gJoofZ94sQJ3XbbbWpubtaxY8e0bNkyrV279rIP1hNJqL0P6+vr04YNG7RkyZIYVRpZ4fS9fv16HTlyRLt27dJnn32m119/XYsWLYph1eMXat8dHR0qLS1VcXGxPB6Pmpqa5PV69eCDD8a48vEZGBhQVlaWtm/frhkzZvzn/n/88YdWrVqllJQUtbW1afv27aqrq4urdVEikTOmZkYwkzNkJFPzZCymZkwwUzMnWCwzaFJfX198fN0WosLCQi1atEi7d+8ObHM4HFq5cqW2bNkyav8tW7bovffeU2dnZ2DbI488Iq/Xqw8//DAmNUdCqH2PZenSpcrNzdW2bduiVWZUhNv72rVrlZ2dLb/fr5aWFnk8nliUGzGh9t3W1qb7779fJ0+eVFJSUixLjahQ+66rq9Mrr7yiM2fOBLa99dZbevLJJ3Xu3LmY1Bxpc+bM0XPPPac1a9b84z6vvvqqtm7dqq+//joQOC6XS42Njfryyy95Itf/mZoZwUzOkJFMzZOxmJoxwcic0aKdQZafibGCHRbajIZw+h5Lf3+/EhMTI11eVIXb+759+9Tb26vy8vJolxgV4fTd2tqqnJwcvfTSS8rKypLD4VBFRYX6+/tjUXJEhNP34sWL5fP59MEHH8jv9+u3337TO++8o2XLlsWiZMt8+umnys3NveQbs8LCQv3888/6/vvvLaxs4jA1M4KZnCEjmZonYzE1Y4KROeEbTwYZOcSYugBaOH0Ha2ho0E8//aR77703GiVGTTi9d3V1qba2Vnv37tXkyZNjUWbEhdP3d999p46ODp05c0ZvvPGGXC6Xjhw5ooceeigWJUdEOH07nU7t27dPJSUlSk5OVnp6uvx+v/bs2ROLki3zT8e24ddgbmYEMzlDRjI1T8ZiasYEI3PCN54MMnKIGWbqAmih9j3s0KFDqqqq0t69ewM3xdrN5fb+559/av369XrmmWc0f/78GFUXPaF85n/99ZcmTZqkhoYG3XLLLSosLJTL5VJLS4vt/qkNpW+v16tNmzapvLxcR48e1YEDB+Tz+bRx48ZYlGqpeDm2RZupmRHM5AwZydQ8GYupGROMzAlPuMfKKVGraAIzdQG0cPoedujQIW3YsEH19fVavnx5NMuMilB77+npkdfrVVlZmcrKyiT9feD1+/1KSkrS/v37R502nojC+cxnz56ttLQ0JSQkBLYtXLhQ0t9PckpJSYlewRESTt87d+6Uw+HQo48+KknKzs7WFVdcobvuukubN2/W3Llzo163Ff7p2CbpP48LpjA1M4KZnCEjmZonYzE1Y4KROeEbTwYZeSbG1AXQwulbkg4ePKjS0lK9/PLLYT/O2mqh9n7NNdfI7Xbr+PHjgZ9169bp+uuv1/Hjx+V0OmNV+riE85nfeuut6unpueT65O7ubknSvHnzoldsBIXT9+Dg4KjLPIZ/j5fHDY/F6XTK4/Ho/PnzgW3t7e1KS0vTddddZ2FlE4epmRHM5AwZydQ8GYupGROMzAnfeDLI0sUurWTqAmih9n3gwAGVlJSourpad955pwYGBjQwMKCLFy9e1qPzJpJQep88ebKSk5Mv+ens7FR3d7cqKys1bdo0q9u5bKF+5jfccIOampp06tQpZWRkqLu7W+Xl5crLy/vXJ4xMNKH2PTg4qLq6OiUlJWnWrFmBU/2zZ8/WY489ZnE3l6+/v19er1c+n09vvvmmsrKyNHPmTF24cEEJCQmqrq7Wzp07VVxcLElKT0/Xa6+9ptOnT+vGG2+Ux+NRVVWVNm7c+K//mJrG1MwIZnKGjGRqnozF1IwJZmrmBItlBhl5OZn09wJov//+u1wul3w+nzIzM0ctgDbS8AJoTz31lBobG5WamjquhTatEmrfjY2NGhoaUmVlpSorKwPb8/LyRq2tMNGF2nu8CLXvK6+8Uu+++64qKiq0dOlSJSYmasWKFZf9+NSJItS+16xZo/7+fjU0NOjpp5/WzJkzVVBQoOrqaivKD9vJkycDISlJNTU1qqmpUXFxsfbs2aOenh59++23gdcTEhJ08OBBPfHEE7rjjjuUmJiosrIyPfzww1aUP2GZmhnBTM6QkUzNk7GYmjHBTM2cYLHMIGPXiQEAAABgT0beEwMAAADAvhhiAAAAANgKQwwAAAAAW2GIAQAAAGArDDEAAAAAbIUhBgAAAICtMMQAAAAAsBWGGAAAAAC2whADAAAAwFYYYgAAAADYCkMMEGODg4NyOp1yOBwaGBgIbB8YGFBOTo6cTqfOnz9vYYUAADsjZ2AChhggxmbMmKH6+nr98MMPqqqqCmzfvHmzfvzxR9XX12v69OkWVggAsDNyBiaYYnUBgIkcDocef/xxuVwurVixQpLU2NioiooKORwOi6sDANgdOYN4N6mvr89vdRGAiS5evKiioiL9+uuv8vv9Sk5O1kcffaSpU6daXRoAIA6QM4hnDDGAhbq6upSXl6cpU6bok08+UUZGhtUlAQDiCDmDeMU9MYCF2traJElDQ0P66quvLK4GABBvyBnEK87EABbxer1asmSJ7r77bp07d07ffPONPB6PkpOTrS4NABAHyBnEM4YYwAJDQ0MqKiqSz+eT2+1WX1+f8vPzdfvtt6upqcnq8gAANkfOIN5xORlggR07dujUqVPatWuXrr76ai1YsEDV1dVqbW3V22+/bXV5AACbI2cQ7zgTA8TY559/rqKiIhUXF2v37t2B7X6/X6tXr1ZnZ6fcbrfmzJljYZUAALsiZ2AChhgAAAAAtsLlZAAAAABshSEGAAAAgK0wxAAAAACwFYYYAAAAALbCEAMAAADAVhhiAAAAANgKQwwAAAAAW2GIAQAAAGArDDEAAAAAbIUhBgAAAICt/A+ONDcznI3AFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code for generating Figure 1\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].scatter(x_train, y_train)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_ylim([0, 3])\n",
    "ax[0].set_title('Generated Data - Train')\n",
    "ax[1].scatter(x_val, y_val, c='r')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_ylim([0, 3])\n",
    "ax[1].set_title('Generated Data - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Model's Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7421577700550976\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Computing the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train - yhat)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.044811379650508 -1.8337537171510832\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "b_grad = -2 * error.mean()\n",
    "w_grad = -2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[0.80119529] [0.04511107]\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "print(b, w)\n",
    "\n",
    "# Step 4 - Updates parameters using gradients and the learning rate\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to Step 1 and run observe how your parameters b and w change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[1.02354094] [1.96896411]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    b_grad = -2 * error.mean()\n",
    "    w_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[ 0.2101,  0.1259, -1.3825, -0.6931],\n",
      "         [-1.6818, -0.3078, -0.2322, -1.2309],\n",
      "         [-1.0997,  0.3713, -0.9452, -0.4315]],\n",
      "\n",
      "        [[-0.0613,  0.4726,  0.1652,  0.2902],\n",
      "         [ 0.3317,  0.4283,  0.0087, -1.7977],\n",
      "         [-0.1552, -0.0509, -0.6905,  0.3240]]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(scalar.size(), scalar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# We get a tensor with a different shape but it still is the SAME tensor\n",
    "same_matrix = matrix.view(1, 6)\n",
    "# If we change one of its elements...\n",
    "same_matrix[0, 1] = 2.\n",
    "# It changes both variables: matrix and same_matrix\n",
    "print(matrix)\n",
    "print(same_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 3., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvgodoy/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# We can use \"new_tensor\" method to REALLY copy it into a new one\n",
    "different_matrix = matrix.new_tensor(matrix.view(1, 6))\n",
    "# Now, if we change one of its elements...\n",
    "different_matrix[0, 1] = 3.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "# But we get a \"warning\" from PyTorch telling us to use \"clone()\" instead!\n",
    "print(matrix)\n",
    "print(different_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 4., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Lets follow PyTorch's suggestion and use \"clone\" method\n",
    "another_matrix = matrix.view(1, 6).clone().detach()\n",
    "# Again, if we change one of its elements...\n",
    "another_matrix[0, 1] = 4.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "print(matrix)\n",
    "print(another_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype, x_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor = x_train_tensor.float()\n",
    "float_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "# Modifies the numpy array\n",
    "dummy_array[1] = 0\n",
    "# Tensor gets modified too...\n",
    "dummy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7713], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tensor = torch.as_tensor(x_train).to(device)\n",
    "gpu_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them \n",
    "# into PyTorch's Tensors and then we send them to the chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_to_numpy = x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_to_numpy = x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"b\" and \"w\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(b, w)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "b.requires_grad_()\n",
    "w.requires_grad_()\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FINAL\n",
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "\n",
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train_tensor - yhat)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "# No more manual computation of gradients! \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(error.requires_grad, yhat.requires_grad, b.requires_grad, w.requires_grad)\n",
    "print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.1125]) tensor([-1.8156])\n"
     ]
    }
   ],
   "source": [
    "print(b.grad, w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the two cells above one more time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e6869e9d7aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This code will be placed *after* Step 4 (updating the parameters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "# This code will be placed *after* Step 4 (updating the parameters)\n",
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = y_train_tensor - yhat\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    # No more manual computation of gradients! \n",
    "    # b_grad = -2 * error.mean()\n",
    "    # w_grad = -2 * (x_tensor * error).mean()   \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad\n",
    "    # w = w - lr * w.grad\n",
    "    # print(b)\n",
    "\n",
    "    # SECOND ATTEMPT - using in-place Python assigment\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # b -= lr * b.grad\n",
    "    # w -= lr * w.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we used in the THIRD ATTEMPT..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 140386046182352 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140386046182352</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036118672 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140386036118672</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140386036118672&#45;&gt;140386046182352 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140386036118672&#45;&gt;140386046182352</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 140386036120336 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140386036120336</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036120336&#45;&gt;140386046182352 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140386036120336&#45;&gt;140386046182352</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 140387329003216 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140387329003216</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140387329003216&#45;&gt;140386036120336 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140387329003216&#45;&gt;140386036120336</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fae2be42310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = y_train_tensor - yhat\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# We can try plotting the graph for any python variable: yhat, error, loss...\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"100pt\" height=\"157pt\"\n",
       " viewBox=\"0.00 0.00 100.00 157.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 153)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-153 96,-153 96,4 -4,4\"/>\n",
       "<!-- 140386037710736 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140386037710736</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"92,-21 0,-21 0,0 92,0 92,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140386037710416 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140386037710416</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"91.5,-78 .5,-78 .5,-57 91.5,-57 91.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140386037710416&#45;&gt;140386037710736 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140386037710416&#45;&gt;140386037710736</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-56.7787C46,-49.6134 46,-39.9517 46,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-31.1732 46,-21.1732 42.5001,-31.1732 49.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140386036179792 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140386036179792</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"73,-149 19,-149 19,-114 73,-114 73,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140386036179792&#45;&gt;140386037710416 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140386036179792&#45;&gt;140386037710416</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-113.6724C46,-105.8405 46,-96.5893 46,-88.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-88.2234 46,-78.2234 42.5001,-88.2235 49.5001,-88.2234\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fae2c48c610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b_nograd + w * x_train_tensor\n",
    "\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"284pt\" height=\"399pt\"\n",
       " viewBox=\"0.00 0.00 284.00 399.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 395)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-395 280,-395 280,4 -4,4\"/>\n",
       "<!-- 140386036179856 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140386036179856</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"215,-21 123,-21 123,0 215,0 215,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"169\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036179024 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140386036179024</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"180,-78 82,-78 82,-57 180,-57 180,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"131\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036179024&#45;&gt;140386036179856 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140386036179024&#45;&gt;140386036179856</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M138.1475,-56.7787C143.2429,-49.1357 150.2317,-38.6524 156.2694,-29.596\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.2497,-31.4352 161.8845,-21.1732 153.4253,-27.5522 159.2497,-31.4352\"/>\n",
       "</g>\n",
       "<!-- 140385972650704 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140385972650704</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159.5,-135 66.5,-135 66.5,-114 159.5,-114 159.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650704&#45;&gt;140386036179024 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140385972650704&#45;&gt;140386036179024</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M116.3857,-113.7787C118.6987,-106.4542 121.8354,-96.5211 124.61,-87.7352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.9557,-88.763 127.6295,-78.1732 121.2806,-86.655 127.9557,-88.763\"/>\n",
       "</g>\n",
       "<!-- 140385972650512 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140385972650512</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"158,-192 68,-192 68,-171 158,-171 158,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650512&#45;&gt;140385972650704 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140385972650512&#45;&gt;140385972650704</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113,-170.7787C113,-163.6134 113,-153.9517 113,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.5001,-145.1732 113,-135.1732 109.5001,-145.1732 116.5001,-145.1732\"/>\n",
       "</g>\n",
       "<!-- 140385972650832 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140385972650832</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159,-249 67,-249 67,-228 159,-228 159,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650832&#45;&gt;140385972650512 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140385972650832&#45;&gt;140385972650512</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113,-227.7787C113,-220.6134 113,-210.9517 113,-202.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.5001,-202.1732 113,-192.1732 109.5001,-202.1732 116.5001,-202.1732\"/>\n",
       "</g>\n",
       "<!-- 140385972650960 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140385972650960</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-320 0,-320 0,-285 54,-285 54,-320\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-292.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140385972650960&#45;&gt;140385972650832 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140385972650960&#45;&gt;140385972650832</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.9558,-284.6724C63.3538,-275.446 78.3987,-264.2498 90.5683,-255.1934\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"92.9097,-257.8138 98.8425,-249.0358 88.7306,-252.1981 92.9097,-257.8138\"/>\n",
       "</g>\n",
       "<!-- 140385972651024 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140385972651024</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-313 72.5,-313 72.5,-292 163.5,-292 163.5,-313\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-299.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972651024&#45;&gt;140385972650832 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140385972651024&#45;&gt;140385972650832</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M117.1744,-291.9317C116.4837,-283.0913 115.4775,-270.2122 114.6261,-259.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.1119,-258.9949 113.8436,-249.2979 111.1332,-259.5402 118.1119,-258.9949\"/>\n",
       "</g>\n",
       "<!-- 140385972651152 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140385972651152</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-391 145,-391 145,-356 199,-356 199,-391\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-363.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140385972651152&#45;&gt;140385972651024 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140385972651152&#45;&gt;140385972651024</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M158.6517,-355.9494C150.6484,-345.4266 140.4734,-332.0484 132.3053,-321.3089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.8474,-318.8695 126.0078,-313.0288 129.2757,-323.1071 134.8474,-318.8695\"/>\n",
       "</g>\n",
       "<!-- 140385972650896 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140385972650896</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272.5,-313 181.5,-313 181.5,-292 272.5,-292 272.5,-313\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-299.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972651152&#45;&gt;140385972650896 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140385972651152&#45;&gt;140385972650896</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M185.5955,-355.9494C193.8285,-345.3214 204.3179,-331.7806 212.6787,-320.9875\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.4868,-323.0778 218.8439,-313.0288 209.9529,-318.7909 215.4868,-323.0778\"/>\n",
       "</g>\n",
       "<!-- 140386036179600 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140386036179600</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"276,-135 178,-135 178,-114 276,-114 276,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036179600&#45;&gt;140386036179856 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140386036179600&#45;&gt;140386036179856</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M221.6475,-113.9795C211.9855,-94.9888 191.4875,-54.6995 179.1119,-30.3751\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"182.1629,-28.6533 174.5088,-21.3276 175.9239,-31.8275 182.1629,-28.6533\"/>\n",
       "</g>\n",
       "<!-- 140385972650576 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140385972650576</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-249 182,-249 182,-228 272,-228 272,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650576&#45;&gt;140386036179600 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140385972650576&#45;&gt;140386036179600</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227,-227.9795C227,-209.242 227,-169.7701 227,-145.3565\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.5001,-145.3276 227,-135.3276 223.5001,-145.3277 230.5001,-145.3276\"/>\n",
       "</g>\n",
       "<!-- 140385972650896&#45;&gt;140385972650576 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140385972650896&#45;&gt;140385972650576</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227,-291.9317C227,-283.0913 227,-270.2122 227,-259.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.5001,-259.2979 227,-249.2979 223.5001,-259.2979 230.5001,-259.2979\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fae2bd07810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b + w * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# this makes no sense!!\n",
    "if loss > 0:\n",
    "    yhat2 = w * x_train_tensor\n",
    "    error2 = y_train_tensor - yhat2\n",
    "    \n",
    "# neither does this :-)\n",
    "loss += error2.mean()\n",
    "\n",
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step / zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = y_train_tensor - yhat\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # No more manual loss!\n",
    "    # error = y_train_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0080, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8664a4f00328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You will NOT get an error here if you do not have a GPU!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# You will NOT get an error here if you do not have a GPU!\n",
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.00804466, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008044655434787273 0.008044655434787273\n"
     ]
    }
   ],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True), Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "dummy = ManualLinearRegression()\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [140387999856752, 140386035993328]}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model and sends it to the device\n",
    "dummy = ManualLinearRegression().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0235], device='cuda:0')), ('w', tensor([1.9690], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # What is this?!?\n",
    "\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Never forget to include model.train() in your training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0.8300], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = MyLinearRegression().to(device)\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')), ('0.bias', tensor([0.8300], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
      "        [ 0.5304, -0.1265,  0.1165],\n",
      "        [-0.2811,  0.3391,  0.5090],\n",
      "        [-0.4236,  0.5018,  0.1081],\n",
      "        [ 0.4266,  0.0782,  0.2784]], device='cuda:0')), ('0.bias', tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')), ('1.weight', tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')), ('1.bias', tensor([-0.2062], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "# If you're running this in Google Colab, it needs to create the folders\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    folders = ['data_preparation', 'model_configuration', 'model_training']\n",
    "\n",
    "    for folder in folders:\n",
    "        try:\n",
    "            os.mkdir(folder)\n",
    "        except OSError as e:\n",
    "            e.errno\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "                \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_preparation/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparation/v0.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i data_preparation/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_configuration/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v0.py\n",
    "\n",
    "# This is redundant now, but it won't be when we introduce Datasets...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v0.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]], device='cuda:0')), ('0.bias', tensor([1.0235], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
