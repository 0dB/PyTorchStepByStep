{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# If you're using Google Colab, please run this #\n",
    "# cell to avoid errors importing torchviz       #\n",
    "#################################################\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Generated Data - Validation')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code for generating Figure 1\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].scatter(x_train, y_train)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_ylim([0, 3])\n",
    "ax[0].set_title('Generated Data - Train')\n",
    "ax[1].scatter(x_val, y_val, c='r')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_ylim([0, 3])\n",
    "ax[1].set_title('Generated Data - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Model's Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7421577700550976\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Computing the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train - yhat)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.044811379650508 -1.8337537171510832\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "b_grad = -2 * error.mean()\n",
    "w_grad = -2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[0.80119529] [0.04511107]\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "print(b, w)\n",
    "\n",
    "# Step 4 - Updates parameters using gradients and the learning rate\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to Step 1 and run observe how your parameters b and w change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[1.02354094] [1.96896411]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    b_grad = -2 * error.mean()\n",
    "    w_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[-0.5096, -1.6260, -2.0673,  1.4963],\n",
      "         [-1.1968,  0.6946, -1.7027,  0.7635],\n",
      "         [-0.2259,  1.9064,  1.5856, -1.0476]],\n",
      "\n",
      "        [[ 0.5527,  1.0779, -0.5784,  1.0472],\n",
      "         [-0.3801,  0.3336, -0.5465, -1.1009],\n",
      "         [-1.7446, -0.6200, -1.1926, -1.4322]]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1709b7421270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You will NOT get an error here if you do not have a GPU!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# You will NOT get an error here if you do not have a GPU!\n",
    "x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"b\" and \"w\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(b, w)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "b.requires_grad_()\n",
    "w.requires_grad_()\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL\n",
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "\n",
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train_tensor - yhat)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "# No more manual computation of gradients! \n",
    "loss.backward()\n",
    "\n",
    "print(b.grad, w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the cell above one more time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e6869e9d7aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This code will be placed *after* Step 4 (updating the parameters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "# This code will be placed *after* Step 4 (updating the parameters)\n",
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = y_train_tensor - yhat\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    # No more manual computation of gradients! \n",
    "    # b_grad = -2 * error.mean()\n",
    "    # w_grad = -2 * (x_tensor * error).mean()   \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad\n",
    "    # w = w - lr * w.grad\n",
    "    # print(b)\n",
    "\n",
    "    # SECOND ATTEMPT - using in-place Python assigment\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # b -= lr * b.grad\n",
    "    # w -= lr * w.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we used in the THIRD ATTEMPT..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 140386046182352 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140386046182352</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036118672 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140386036118672</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140386036118672&#45;&gt;140386046182352 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140386036118672&#45;&gt;140386046182352</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 140386036120336 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140386036120336</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036120336&#45;&gt;140386046182352 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140386036120336&#45;&gt;140386046182352</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 140387329003216 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140387329003216</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140387329003216&#45;&gt;140386036120336 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140387329003216&#45;&gt;140386036120336</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fae2be42310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = y_train_tensor - yhat\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# We can try plotting the graph for any python variable: yhat, error, loss...\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"100pt\" height=\"157pt\"\n",
       " viewBox=\"0.00 0.00 100.00 157.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 153)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-153 96,-153 96,4 -4,4\"/>\n",
       "<!-- 140386037710736 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140386037710736</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"92,-21 0,-21 0,0 92,0 92,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140386037710416 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140386037710416</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"91.5,-78 .5,-78 .5,-57 91.5,-57 91.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140386037710416&#45;&gt;140386037710736 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140386037710416&#45;&gt;140386037710736</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-56.7787C46,-49.6134 46,-39.9517 46,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-31.1732 46,-21.1732 42.5001,-31.1732 49.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140386036179792 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140386036179792</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"73,-149 19,-149 19,-114 73,-114 73,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140386036179792&#45;&gt;140386037710416 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140386036179792&#45;&gt;140386037710416</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-113.6724C46,-105.8405 46,-96.5893 46,-88.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-88.2234 46,-78.2234 42.5001,-88.2235 49.5001,-88.2234\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fae2c48c610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b_nograd + w * x_train_tensor\n",
    "\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"284pt\" height=\"399pt\"\n",
       " viewBox=\"0.00 0.00 284.00 399.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 395)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-395 280,-395 280,4 -4,4\"/>\n",
       "<!-- 140386036179856 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140386036179856</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"215,-21 123,-21 123,0 215,0 215,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"169\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036179024 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140386036179024</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"180,-78 82,-78 82,-57 180,-57 180,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"131\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036179024&#45;&gt;140386036179856 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140386036179024&#45;&gt;140386036179856</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M138.1475,-56.7787C143.2429,-49.1357 150.2317,-38.6524 156.2694,-29.596\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.2497,-31.4352 161.8845,-21.1732 153.4253,-27.5522 159.2497,-31.4352\"/>\n",
       "</g>\n",
       "<!-- 140385972650704 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140385972650704</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159.5,-135 66.5,-135 66.5,-114 159.5,-114 159.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650704&#45;&gt;140386036179024 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140385972650704&#45;&gt;140386036179024</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M116.3857,-113.7787C118.6987,-106.4542 121.8354,-96.5211 124.61,-87.7352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.9557,-88.763 127.6295,-78.1732 121.2806,-86.655 127.9557,-88.763\"/>\n",
       "</g>\n",
       "<!-- 140385972650512 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140385972650512</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"158,-192 68,-192 68,-171 158,-171 158,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650512&#45;&gt;140385972650704 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140385972650512&#45;&gt;140385972650704</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113,-170.7787C113,-163.6134 113,-153.9517 113,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.5001,-145.1732 113,-135.1732 109.5001,-145.1732 116.5001,-145.1732\"/>\n",
       "</g>\n",
       "<!-- 140385972650832 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140385972650832</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159,-249 67,-249 67,-228 159,-228 159,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650832&#45;&gt;140385972650512 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140385972650832&#45;&gt;140385972650512</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113,-227.7787C113,-220.6134 113,-210.9517 113,-202.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.5001,-202.1732 113,-192.1732 109.5001,-202.1732 116.5001,-202.1732\"/>\n",
       "</g>\n",
       "<!-- 140385972650960 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140385972650960</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-320 0,-320 0,-285 54,-285 54,-320\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-292.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140385972650960&#45;&gt;140385972650832 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140385972650960&#45;&gt;140385972650832</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.9558,-284.6724C63.3538,-275.446 78.3987,-264.2498 90.5683,-255.1934\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"92.9097,-257.8138 98.8425,-249.0358 88.7306,-252.1981 92.9097,-257.8138\"/>\n",
       "</g>\n",
       "<!-- 140385972651024 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140385972651024</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-313 72.5,-313 72.5,-292 163.5,-292 163.5,-313\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-299.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972651024&#45;&gt;140385972650832 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140385972651024&#45;&gt;140385972650832</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M117.1744,-291.9317C116.4837,-283.0913 115.4775,-270.2122 114.6261,-259.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.1119,-258.9949 113.8436,-249.2979 111.1332,-259.5402 118.1119,-258.9949\"/>\n",
       "</g>\n",
       "<!-- 140385972651152 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140385972651152</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-391 145,-391 145,-356 199,-356 199,-391\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-363.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140385972651152&#45;&gt;140385972651024 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140385972651152&#45;&gt;140385972651024</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M158.6517,-355.9494C150.6484,-345.4266 140.4734,-332.0484 132.3053,-321.3089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.8474,-318.8695 126.0078,-313.0288 129.2757,-323.1071 134.8474,-318.8695\"/>\n",
       "</g>\n",
       "<!-- 140385972650896 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140385972650896</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272.5,-313 181.5,-313 181.5,-292 272.5,-292 272.5,-313\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-299.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972651152&#45;&gt;140385972650896 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140385972651152&#45;&gt;140385972650896</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M185.5955,-355.9494C193.8285,-345.3214 204.3179,-331.7806 212.6787,-320.9875\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.4868,-323.0778 218.8439,-313.0288 209.9529,-318.7909 215.4868,-323.0778\"/>\n",
       "</g>\n",
       "<!-- 140386036179600 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140386036179600</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"276,-135 178,-135 178,-114 276,-114 276,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140386036179600&#45;&gt;140386036179856 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140386036179600&#45;&gt;140386036179856</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M221.6475,-113.9795C211.9855,-94.9888 191.4875,-54.6995 179.1119,-30.3751\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"182.1629,-28.6533 174.5088,-21.3276 175.9239,-31.8275 182.1629,-28.6533\"/>\n",
       "</g>\n",
       "<!-- 140385972650576 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140385972650576</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-249 182,-249 182,-228 272,-228 272,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140385972650576&#45;&gt;140386036179600 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140385972650576&#45;&gt;140386036179600</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227,-227.9795C227,-209.242 227,-169.7701 227,-145.3565\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.5001,-145.3276 227,-135.3276 223.5001,-145.3277 230.5001,-145.3276\"/>\n",
       "</g>\n",
       "<!-- 140385972650896&#45;&gt;140385972650576 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140385972650896&#45;&gt;140385972650576</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227,-291.9317C227,-283.0913 227,-270.2122 227,-259.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.5001,-259.2979 227,-249.2979 223.5001,-259.2979 230.5001,-259.2979\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fae2bd07810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b + w * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# this makes no sense!!\n",
    "if loss > 0:\n",
    "    yhat2 = w * x_train_tensor\n",
    "    error2 = y_train_tensor - yhat2\n",
    "    \n",
    "# neither does this :-)\n",
    "loss += error2.mean()\n",
    "\n",
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step / zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient descent\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = y_train_tensor - yhat\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"a\" and \"b\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # No more manual loss!\n",
    "    # error = y_train_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0080, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8664a4f00328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You will NOT get an error here if you do not have a GPU!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# You will NOT get an error here if you do not have a GPU!\n",
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.00804466, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008044655434787273 0.008044655434787273\n"
     ]
    }
   ],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True), Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "dummy = ManualLinearRegression()\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [140387999856752, 140386035993328]}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model and sends it to the device\n",
    "dummy = ManualLinearRegression().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0235], device='cuda:0')), ('w', tensor([1.9690], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # What is this?!?\n",
    "\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Never forget to include model.train() in your training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0.8300], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = MyLinearRegression().to(device)\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')), ('0.bias', tensor([0.8300], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
      "        [ 0.5304, -0.1265,  0.1165],\n",
      "        [-0.2811,  0.3391,  0.5090],\n",
      "        [-0.4236,  0.5018,  0.1081],\n",
      "        [ 0.4266,  0.0782,  0.2784]], device='cuda:0')), ('0.bias', tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')), ('1.weight', tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')), ('1.bias', tensor([-0.2062], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "# If you're running this in Google Colab, it needs to create the folders\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    folders = ['data_preparation', 'model_configuration', 'model_training']\n",
    "\n",
    "    for folder in folders:\n",
    "        try:\n",
    "            os.mkdir(folder)\n",
    "        except OSError as e:\n",
    "            e.errno\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "                \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_preparation/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparation/v0.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i data_preparation/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_configuration/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v0.py\n",
    "\n",
    "# This is redundant now, but it won't be when we introduce Datasets...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v0.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1 - Computes our model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]], device='cuda:0')), ('0.bias', tensor([1.0235], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
